<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Merhan Ghandehari" />


<title>Lab 8: Spatial Regression and Poverty</title>

<script src="ghandehari_lab8_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="ghandehari_lab8_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="ghandehari_lab8_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="ghandehari_lab8_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="ghandehari_lab8_files/bootstrap-3.3.5/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="ghandehari_lab8_files/highlight/default.css"
      type="text/css" />
<script src="ghandehari_lab8_files/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>




</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="ghandehari_lab8_files/navigation-1.0/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">


<h1 class="title">Lab 8: Spatial Regression and Poverty</h1>
<h4 class="author"><em>Merhan Ghandehari</em></h4>

</div>


<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>The goal of these exercises is to give you a chance to put the concepts we have been discussing in class into practice. Keep in mind we have only a limited amount of time (one week this time!), so our focus for this lab will be <em>breadth</em> rather than <em>depth</em>!</p>
<p>To get started, let’s load some data. We’re going to use the Southern Counties Data from <a href="http://www.odum.unc.edu/content/pdf/Voss%20et%20al%202006.pdf">Voss (2006)</a>, which we’ve been working with in class. There are two ways to get this data, the ‘traditional’ way (loading a shapefile), and the ‘easy’ way, loading a ‘pre-packaged’ dataset I have prepared for you:</p>
<pre class="r"><code>library(rgdal)
soco = readOGR(&quot;Data/south00.shp&quot;, layer=&quot;south00&quot;)</code></pre>
<pre><code>## OGR data source with driver: ESRI Shapefile 
## Source: &quot;Data/south00.shp&quot;, layer: &quot;south00&quot;
## with 1387 features
## It has 17 fields</code></pre>
<pre class="r"><code>summary(soco@data)</code></pre>
<p>If the above doesn’t work, you can always get the data from here:</p>
<pre class="r"><code>load(&quot;Data/soco.rdata&quot;)
summary(soco@data)  # You might want to check this out</code></pre>
<p>The data contains the following variables/information (this info is also available in the <code>vardescription_south00.csv</code> file). This dataset was originally used to examine intercounty variation in child poverty rates in the US. In particular, the authors used it as a way to demonstate the utility of spatial regression analysis.</p>
<table>
<thead>
<tr class="header">
<th align="left">Variable</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">CNTY_ST</td>
<td align="left">County and state name</td>
</tr>
<tr class="even">
<td align="left">STUSAB</td>
<td align="left">State abbreviation</td>
</tr>
<tr class="odd">
<td align="left">FIPS</td>
<td align="left">FIPS code</td>
</tr>
<tr class="even">
<td align="left">YCOORD</td>
<td align="left">Y coordinate (meters)</td>
</tr>
<tr class="odd">
<td align="left">XCOORD</td>
<td align="left">X coordinate (meters)</td>
</tr>
<tr class="even">
<td align="left">SQYCORD</td>
<td align="left">Y coordinate squared (for trend surface)</td>
</tr>
<tr class="odd">
<td align="left">SQXCORD</td>
<td align="left">X coordinate squared (for trend surface)</td>
</tr>
<tr class="even">
<td align="left">XYCOORD</td>
<td align="left">X coordinate * Y coordinate (for trend surface)</td>
</tr>
<tr class="odd">
<td align="left">PPOV</td>
<td align="left">Proportion of children in poverty</td>
</tr>
<tr class="even">
<td align="left">PHSP</td>
<td align="left">Proportion Hispanic</td>
</tr>
<tr class="odd">
<td align="left">PFHH</td>
<td align="left">Proportion female-headed households</td>
</tr>
<tr class="even">
<td align="left">PWKCO</td>
<td align="left">Proportion work outside of county of residence</td>
</tr>
<tr class="odd">
<td align="left">PHSLS</td>
<td align="left">Proportion less than high school educated</td>
</tr>
<tr class="even">
<td align="left">PUNEM</td>
<td align="left">Proportion unemployed</td>
</tr>
<tr class="odd">
<td align="left">PUDEM</td>
<td align="left">Proportion males underemployed, some work in 1999</td>
</tr>
<tr class="even">
<td align="left">PEXTR</td>
<td align="left">Proportion employed in extractive industry</td>
</tr>
<tr class="odd">
<td align="left">PPSRV</td>
<td align="left">Proportion employed in professional services</td>
</tr>
<tr class="even">
<td align="left">PMSRV</td>
<td align="left">Proportion employed in miscellaneous services</td>
</tr>
<tr class="odd">
<td align="left">PNDMFG</td>
<td align="left">Proportion employed in non-durable manufacturing</td>
</tr>
<tr class="even">
<td align="left">PNHSPW</td>
<td align="left">Proportion non-Hispanic white</td>
</tr>
<tr class="odd">
<td align="left">PMNRTY</td>
<td align="left">Proportion minority (total - non-Hispanic white)</td>
</tr>
<tr class="even">
<td align="left">LO_POV</td>
<td align="left">Natural log proportion children in poverty</td>
</tr>
<tr class="odd">
<td align="left">PFRN</td>
<td align="left">Proportion foreign-born</td>
</tr>
<tr class="even">
<td align="left">PNAT</td>
<td align="left">Proportion native-born</td>
</tr>
<tr class="odd">
<td align="left">PBLK</td>
<td align="left">Proportion African American/black, alone and including Hispanic</td>
</tr>
<tr class="even">
<td align="left">P65UP</td>
<td align="left">Proportion 65 and older</td>
</tr>
<tr class="odd">
<td align="left">PDSABL</td>
<td align="left">Proportion disabled</td>
</tr>
<tr class="even">
<td align="left">METRO</td>
<td align="left">Metro county</td>
</tr>
<tr class="odd">
<td align="left">PERPOV</td>
<td align="left">Persistent poverty, 1970-2000 (ERS)</td>
</tr>
<tr class="even">
<td align="left">OTMIG</td>
<td align="left">Rate of out-migration</td>
</tr>
<tr class="odd">
<td align="left">BINMIG</td>
<td align="left">Rate of black in-migration from non-south</td>
</tr>
<tr class="even">
<td align="left">INCARC</td>
<td align="left">Proportion males 18-64 in correctional institutions</td>
</tr>
<tr class="odd">
<td align="left">BINCARC</td>
<td align="left">Proportion black males 18-64 in correctional institutions</td>
</tr>
<tr class="even">
<td align="left">SQRTPPOV</td>
<td align="left">Square root proportion children in poverty</td>
</tr>
<tr class="odd">
<td align="left">SQRTUNEM</td>
<td align="left">Square root proportion unemployed</td>
</tr>
<tr class="even">
<td align="left">SQRTPFHH</td>
<td align="left">Square root proportion female-headed households</td>
</tr>
<tr class="odd">
<td align="left">LOGHSPLS</td>
<td align="left">Natural log proportion less than high school educated</td>
</tr>
<tr class="even">
<td align="left">PHSPLUS</td>
<td align="left">Natural log proportion high school educated or more</td>
</tr>
</tbody>
</table>
</div>
<div id="part-i-exploratory-spatial-data-analysis" class="section level1">
<h1>Part I: Exploratory Spatial Data Analysis</h1>
<p>The response variable we are interested in is <code>PPOV</code>, which describes the proportion of children living in poverty in each county. So first things first, create a choropleth map that shows the distribution of <code>PPOV</code> across the sothern states:</p>
<pre class="r"><code>library(maptools)
library(RColorBrewer)
library(classInt)
# Consider using quantiles class breaks, or a &#39;ramp-style&#39; palette
# Show R code to produce a choropleth map of PPOV

# palette = colorRampPalette(c(&quot;lightblue&quot;, &quot;darkblue&quot;), space = &quot;rgb&quot;)
# spplot(soco, &quot;PPOV&quot;, col.regions=palette(20), 
#        main=&quot;Distribution of poverty across the sothern states&quot;)


# Create Categories based on quantiles
# cats7 = classIntervals(soco$PPOV, n=7, style=&quot;quantile&quot;)
# pal7 = brewer.pal(7, &quot;YlGnBu&quot;) 
# seven_cols = findColours(cats7, pal7) 
# plot(soco, col=seven_cols, lty=0)


library(ggplot2)  # Plotting and stuff
#soco@data$id &lt;- as.numeric(rownames(soco@data))-1
class(soco)  # Not a regular data.frame...</code></pre>
<pre><code>## [1] &quot;SpatialPolygonsDataFrame&quot;
## attr(,&quot;package&quot;)
## [1] &quot;sp&quot;</code></pre>
<pre class="r"><code>new_map = fortify(soco, region=&quot;FIPS&quot;)
class(new_map)  # Now we have a regular data.frame!</code></pre>
<pre><code>## [1] &quot;data.frame&quot;</code></pre>
<pre class="r"><code>new_map&lt;-merge(new_map,soco@data,by.x=&quot;id&quot;,by.y=&quot;FIPS&quot;) 

p = ggplot(data=new_map, aes(x=long, y=lat, group=group))
p + geom_polygon(aes(fill=PPOV)) + coord_equal() + labs(x=&quot;Longitude&quot;, y=&quot;Latitude&quot;, title=&quot;Proportion of poverty across the sothern states&quot;)  + scale_fill_gradient(&#39;PPOV&#39;, low = &quot;white&quot;, high = &quot;red&quot;, space = &quot;Lab&quot;, na.value = &quot;grey50&quot;, guide = &quot;colourbar&quot;)</code></pre>
<p><img src="ghandehari_lab8_files/figure-html/unnamed-chunk-5-1.png" title="" alt="" width="672" /></p>
<p><code>R</code> doesn’t have a ton of fancy tools for mapping, but it does a pretty good job of quickly giving us the information we will need to do our due diligence on the relationships in our data. If this were a real project then we would need to undertake the above steps for all of our independent variables, but for now we will just proceed with <code>PPOV</code> (our response; done already) and <code>PFHH</code>. In addition to the plots, note if there is any observable spatial co-variation in these variables:</p>
<pre class="r"><code># Show R code to produce choropleth map of PFHH variable
p = ggplot(data=new_map, aes(x=long, y=lat, group=group))
p + geom_polygon(aes(fill=PFHH)) + coord_equal() + labs(x=&quot;Longitude&quot;, y=&quot;Latitude&quot;, title=&quot;Proportion female-headed households across the sothern states&quot;)  + scale_fill_gradient(&#39;PPOV&#39;, low = &quot;white&quot;, high = &quot;red&quot;, space = &quot;Lab&quot;, na.value = &quot;grey50&quot;, guide = &quot;colourbar&quot;)</code></pre>
<p><img src="ghandehari_lab8_files/figure-html/unnamed-chunk-6-1.png" title="" alt="" width="672" /></p>
<pre class="r"><code># Is there any notable co-variation in the above two variables
# Yes, some similar paterns can be seen all over place</code></pre>
<p>It is also always important to examine the underlying attribute distribution of your variables (i.e., via histograms, density plots, etc). Create histograms and any other relevant plots to give us an idea of the underlying distribution of the <code>PPOV</code> and <code>PFHH</code> variables from our dataset. What do you notice about these variables?</p>
<pre class="r"><code># Show R code to produce histograms of the `PPOV` and `PFHH` variables
g = ggplot( data = soco@data, aes( x = PPOV ))
g + geom_histogram(bins = 50, col = &#39;blue&#39;) </code></pre>
<p><img src="ghandehari_lab8_files/figure-html/unnamed-chunk-7-1.png" title="" alt="" width="672" /></p>
<pre class="r"><code>g = ggplot( data = soco@data, aes( x = PFHH ))
g + geom_histogram(bins = 50, col = &#39;blue&#39;) </code></pre>
<p><img src="ghandehari_lab8_files/figure-html/unnamed-chunk-7-2.png" title="" alt="" width="672" /></p>
<pre class="r"><code># The variables PPOV and PFHH pretty much the same distribution</code></pre>
<p>Now that we have an idea of the spatial and aspatial distribution of these variables, it is time to look at their relationships with each other. Create a scatterplot of <code>PPOV</code> with <code>PFHH</code> and note any potentially important relationships you observe:</p>
<pre class="r"><code># Show R code to produce a scatterplot of `PPOV` with `PFHH`
r = ggplot( data = soco@data, aes( x = PFHH, y = PPOV ))
r + geom_point() </code></pre>
<p><img src="ghandehari_lab8_files/figure-html/unnamed-chunk-8-1.png" title="" alt="" width="672" /></p>
<pre class="r"><code>#There is a strong linear positive relationship between these two variable</code></pre>
<p>Take the opportunity here to study the relationship between these two key variables. Now add a ‘best fit’ line to the plot (think <code>abline</code> function or <code>geom_abline</code> if you’re using <code>ggplot</code>). What is the slope of this line?</p>
<pre class="r"><code># Show R code to produce a scatterplot of `PPOV` with `PFHH` with a best fit line
# What is the slope of the line? You&#39;ll need to fit a model here...
PPOV_PFHH.lm = lm(soco@data$PPOV ~ soco@data$PFHH)
coeffs = coefficients(PPOV_PFHH.lm); 
#the slope of line is
coeffs[2]</code></pre>
<pre><code>## soco@data$PFHH 
##      0.7088505</code></pre>
<pre class="r"><code>r = ggplot( data = soco@data, aes( x = PFHH, y = PPOV ))
r + geom_point() + geom_abline(aes(slope=coeffs[2],intercept=coeffs[1]), colour=&quot;red&quot;, size=0.9)</code></pre>
<p><img src="ghandehari_lab8_files/figure-html/unnamed-chunk-9-1.png" title="" alt="" width="672" /></p>
<p>Obviously the two variables are positively and fairly strongly correlated. Does it look like there are any outliers here? How might you determine this (no need to show <code>R</code> code here, just make suggestions)?</p>
<pre class="r"><code># How might you determine if there are outliers in the previous scatterplot?
#yes, there are some points that are far form the fitted line. My suggetion is making a confidence interval aorund the fitted line or using the box plot
boxplot(soco@data$PFHH)</code></pre>
<p><img src="ghandehari_lab8_files/figure-html/unnamed-chunk-10-1.png" title="" alt="" width="672" /></p>
<pre class="r"><code>boxplot(soco@data$PPOV)</code></pre>
<p><img src="ghandehari_lab8_files/figure-html/unnamed-chunk-10-2.png" title="" alt="" width="672" /></p>
<pre class="r"><code># Also I ran the Bonferroni test to assesse Outliers. This test identify two outliers
library(car) 
outlierTest(PPOV_PFHH.lm)</code></pre>
<pre><code>##      rstudent unadjusted p-value Bonferonni p
## 1193 5.320031         1.2087e-07   0.00016765
## 467  4.533546         6.3008e-06   0.00873930</code></pre>
<p>Bonus: try to take a look at the state-specific relationships between <code>PPOV</code> and <code>PFHH</code>. Does this seem to capture any spatial effects?</p>
<pre class="r"><code># Bonus: Show R code to produce conditional scatterplot of `PPOV` with `PFHH` for each state.
# You&#39;ll probably need ggplot for this (facet_wrap)
# In most of the states we can see that these variables are positively and fairly strongly correlated, but in KY, OK, TN, TX and WA correlation is quite weak. In DC and DE also we do not have enough data to make any conclusion.
p = ggplot(data=new_map, aes(x = PFHH, y = PPOV))
p + geom_point(size=.2, alpha = 0.2) + facet_wrap(~ STUSAB)</code></pre>
<p><img src="ghandehari_lab8_files/figure-html/unnamed-chunk-11-1.png" title="" alt="" width="672" /></p>
<div id="ols-and-residuals" class="section level2">
<h2>OLS and Residuals</h2>
<p>At this point we should have a good feel for some of the descriptive characteristics in our data. There is obviously much more that could be done, but you should have the basic idea at this point. Since some of the things we should have done (Global and Local Spatial Autocorrelation) will be done later in the lab anyway, so we can safely skip over them here. For now though, <code>R</code> is just one more software program that will allow you to run a basic OLS Regression.</p>
<div id="spatial-weights" class="section level3">
<h3>Spatial Weights</h3>
<p>Before we get started we need to define a spatial weights matrix. We won’t use it directly here (as we will when we run spatial regression), but <code>R</code>’s <code>spdep</code> allows us to compute Moran’s I for our residuals as well as some other spatial diagnostics on the OLS so we do our matrix now rather than later.</p>
<pre class="r"><code>library(spdep)
w_nb = poly2nb(soco, row.names=soco$FIPS, queen=TRUE)  # Use queen contiguity</code></pre>
<p>Summarize the above neighbors list, and create a simple plot to show overall connectivity. Does this look about right? Which location is the most connected? Bonus: Can you tell me which county this is?</p>
<pre class="r"><code># Show R code to summarize and plot the above neighbors list
summary(w_nb)</code></pre>
<pre><code>## Neighbour list object:
## Number of regions: 1387 
## Number of nonzero links: 7996 
## Percentage nonzero weights: 0.4156424 
## Average number of links: 5.76496 
## Link number distribution:
## 
##   1   2   3   4   5   6   7   8   9  10  11 
##   3  20  46 150 313 460 294  83  13   4   1 
## 3 least connected regions:
## 48141 51131 54029 with 1 link
## 1 most connected region:
## 13107 with 11 links</code></pre>
<pre class="r"><code># Bonus: Which county is the most connected. Tell me the FIPS _and_ name...
#The most connected reagion is Emanuel County in GA with 11 links

library(spdep)
map_crd &lt;- coordinates(soco)
plot(soco, pch=19, cex=0.1, col=&quot;grey&quot;, border=NA)
plot(w_nb,coords=map_crd,pch=19, cex=0.1, lwd=0.2, col=&quot;red&quot;, add= TRUE)</code></pre>
<p><img src="ghandehari_lab8_files/figure-html/unnamed-chunk-13-1.png" title="" alt="" width="672" /></p>
<p>This neighbors list is just one ‘part’ of weights matrix creation. Convert the above neighbors list into a proper weights object, using row standardised weights (<code>see ?nb2listw</code>):</p>
<pre class="r"><code># Show R code to convert w_nb to a weights matrix (call it w_mat)
w_mat &lt;- nb2listw(w_nb, style=&quot;W&quot;, zero.policy=TRUE) # create a spatial weights matrix</code></pre>
</div>
<div id="ols-regression" class="section level3">
<h3>OLS Regression</h3>
<p>Now we’ll fit an OLS Regression model (call it <code>mod1</code>) using the Southern Counties dataset. Firstly, the <em>dependent variable</em> will be the square root of the percentage of children living in poverty (<code>SQRTPPOV</code>), and your <em>independent variables</em> will include <code>PHSP</code>, <code>PFHH</code>, <code>PUNEM</code>, <code>PEXTR</code>, <code>P65UP</code>, <code>METRO</code>, <code>PHSPLUS</code>. Fow now, we’ll ignore any possible interaction terms etc…</p>
<p>What is the <span class="math">\(R^2\)</span> for this model? Are the coefficients all significant?</p>
<pre class="r"><code># Show R code to fit an OLS Regression model with the above variables
SQRTPPOV = sqrt((soco@data$PPOV))
mod1 = lm(sqrt((PPOV)*100)  ~ PHSP + PFHH + PUNEM + PEXTR + P65UP + METRO + PHSPLUS, data = soco@data)
summary(mod1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = sqrt((PPOV) * 100) ~ PHSP + PFHH + PUNEM + PEXTR + 
##     P65UP + METRO + PHSPLUS, data = soco@data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.26116 -0.28304  0.00346  0.28299  2.01636 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  3.09362    0.09790  31.598  &lt; 2e-16 ***
## PHSP         0.71079    0.09903   7.177 1.16e-12 ***
## PFHH         5.29256    0.21091  25.093  &lt; 2e-16 ***
## PUNEM       14.60965    0.63401  23.043  &lt; 2e-16 ***
## PEXTR        3.44671    0.25379  13.581  &lt; 2e-16 ***
## P65UP        2.21940    0.35748   6.208 7.07e-10 ***
## METRO       -0.10478    0.03193  -3.281  0.00106 ** 
## PHSPLUS     -2.83509    0.13695 -20.702  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4544 on 1379 degrees of freedom
## Multiple R-squared:  0.7797, Adjusted R-squared:  0.7786 
## F-statistic: 697.3 on 7 and 1379 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code># What is the R^2 value? Are the coefficients significant?
#The R^2 is 0.7797 that is considered quite high. The coefficients are also significant as they all have small p-value</code></pre>
<p>This leaves us to interpret the various output statistics. Some things to pay attention to: * Log likelihood: higher, better, (less negative) * Aikake info criterion (AIC): lower, better * Others?</p>
<p>These are all aspatial diagnostics and mostly they will give us information in a comparative sense. We should also look for multicollinearity; what is a good test for this (remember, multicolinearity inflates the standard errors (variance) of the coefficients)? Perform this test; is multicolinearity going to be a problem here?</p>
<pre class="r"><code>library(car)
# Show R code to run a test for the effects (variance inflation) of multicolinearity
# Evaluate Collinearity
#The cuttoff is 2.5. So here we do not have collinear variables
vif(mod1) # variance inflation factors </code></pre>
<pre><code>##     PHSP     PFHH    PUNEM    PEXTR    P65UP    METRO  PHSPLUS 
## 1.315538 1.540307 1.683439 1.589017 1.238624 1.611275 1.368839</code></pre>
<p>What about heteroscedasticity (non-constant variance of the residuals)? There is a nice test for this too (<code>ncvTest</code> from the <code>car</code> package), is it significant here? Note: this test is sometimes also called the Breusch-Pagan test.</p>
<pre class="r"><code># Show R code to run ncvTest and determine if it is significant
# P-value is quite small and so the it is significant
library(lmtest)
bptest(mod1)</code></pre>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  mod1
## BP = 85.846, df = 7, p-value = 8.799e-16</code></pre>
<pre class="r"><code>ncvTest(mod1)</code></pre>
<pre><code>## Non-constant Variance Score Test 
## Variance formula: ~ fitted.values 
## Chisquare = 30.19656    Df = 1     p = 3.904003e-08</code></pre>
<p>The potential problems we have with heteroskedasticity could be ameliorated by transforming some of our variables, and/or reducing the impact of certain outliers. Whether or not your discipline thinks this is appropriate statistical technique is up to you. For now we will leave this be, but recall from the lecture that heteroskedasticity is a violation of one of the key assumptions of OLS and its presence should throw into question the validity of your model.</p>
<p>What about the residuals, are they normally distributed? What are some good ways to test this: visually and statistically (the <code>moments</code> package has some useful tests for this)?</p>
<pre class="r"><code># R code to test for normally distributed errors (plot and/or test statistic)
# based on this histgram the residuals nearly have a normal distribution
library(MASS)
sresid &lt;- studres(mod1) 
hist(sresid, freq=FALSE, 
   main=&quot;Distribution of Studentized Residuals&quot;)
xfit&lt;-seq(min(sresid),max(sresid),length=40) 
yfit&lt;-dnorm(xfit) 
lines(xfit, yfit)</code></pre>
<p><img src="ghandehari_lab8_files/figure-html/unnamed-chunk-18-1.png" title="" alt="" width="672" /></p>
<pre class="r"><code> #qq plot for studentized resid
# Based on the qq plot the the residuals have a normal distribution. We have a set of outliers too.
qqPlot(mod1, main=&quot;QQ Plot&quot;)</code></pre>
<p><img src="ghandehari_lab8_files/figure-html/unnamed-chunk-18-2.png" title="" alt="" width="672" /></p>
<pre class="r"><code>#Null hypothesis residuals are normally distributed 
# The null hypothesis can be rejected based on this test. But it seems this happened because there are some outliers.
shapiro.test(resid(mod1))</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  resid(mod1)
## W = 0.9881, p-value = 3.204e-09</code></pre>
</div>
<div id="diagnostics" class="section level3">
<h3>Diagnostics</h3>
<p>Now its time to decide if we need to pursue spatial regression models to counteract any of the issues we’ve observed so far. We can use Anselin’s decision tree framework to help us here:</p>
<p>Firstly, we’ll explore Moran’s I calculated on the OLS residuals. In this example, what does this show?</p>
<pre class="r"><code># Show R code to run a Global Moran&#39;s I test for residual spatial autocorrelation
plot(residuals(mod1), fitted(mod1))</code></pre>
<p><img src="ghandehari_lab8_files/figure-html/unnamed-chunk-19-1.png" title="" alt="" width="672" /></p>
<pre class="r"><code>lm.morantest(mod1,w_mat) #Moran&#39;s I test for residual spatial autocorrelation</code></pre>
<pre><code>## 
##  Global Moran I for regression residuals
## 
## data:  
## model: lm(formula = sqrt((PPOV) * 100) ~ PHSP + PFHH + PUNEM +
## PEXTR + P65UP + METRO + PHSPLUS, data = soco@data)
## weights: w_mat
## 
## Moran I statistic standard deviate = 19.381, p-value &lt; 2.2e-16
## alternative hypothesis: greater
## sample estimates:
## Observed Moran I      Expectation         Variance 
##     0.3081320277    -0.0030597865     0.0002578173</code></pre>
<pre class="r"><code># The Moran&#39;s I value is 0.308 with very samll p-value and therefore significant. So the the residuals are correlated</code></pre>
<p>We can also perform the Lagrange Multiplier tests. These help us to calculate the ‘effectiveness’ of the two forms of spatial regression model, along with their <em>robust</em> forms. The way we read this is we look to see if the lag LM is significant. Then we look at the error LM. If only one is significant then the metrics point to that type of model. If both are significant then we forget what we just read and pick the higher of the two robust scores. Run the <code>lm.LMtests</code> to help us decide which spatial regression model to use going forward:</p>
<pre class="r"><code>#  this test is significant
lm.LMtests(mod1,w_mat, test=&quot;all&quot;)</code></pre>
<pre><code>## 
##  Lagrange multiplier diagnostics for spatial dependence
## 
## data:  
## model: lm(formula = sqrt((PPOV) * 100) ~ PHSP + PFHH + PUNEM +
## PEXTR + P65UP + METRO + PHSPLUS, data = soco@data)
## weights: w_mat
## 
## LMerr = 362.58, df = 1, p-value &lt; 2.2e-16
## 
## 
##  Lagrange multiplier diagnostics for spatial dependence
## 
## data:  
## model: lm(formula = sqrt((PPOV) * 100) ~ PHSP + PFHH + PUNEM +
## PEXTR + P65UP + METRO + PHSPLUS, data = soco@data)
## weights: w_mat
## 
## LMlag = 300.35, df = 1, p-value &lt; 2.2e-16
## 
## 
##  Lagrange multiplier diagnostics for spatial dependence
## 
## data:  
## model: lm(formula = sqrt((PPOV) * 100) ~ PHSP + PFHH + PUNEM +
## PEXTR + P65UP + METRO + PHSPLUS, data = soco@data)
## weights: w_mat
## 
## RLMerr = 132.94, df = 1, p-value &lt; 2.2e-16
## 
## 
##  Lagrange multiplier diagnostics for spatial dependence
## 
## data:  
## model: lm(formula = sqrt((PPOV) * 100) ~ PHSP + PFHH + PUNEM +
## PEXTR + P65UP + METRO + PHSPLUS, data = soco@data)
## weights: w_mat
## 
## RLMlag = 70.706, df = 1, p-value &lt; 2.2e-16
## 
## 
##  Lagrange multiplier diagnostics for spatial dependence
## 
## data:  
## model: lm(formula = sqrt((PPOV) * 100) ~ PHSP + PFHH + PUNEM +
## PEXTR + P65UP + METRO + PHSPLUS, data = soco@data)
## weights: w_mat
## 
## SARMA = 433.29, df = 2, p-value &lt; 2.2e-16</code></pre>
</div>
<div id="residual-maps" class="section level3">
<h3>Residual Maps</h3>
<p>The Mississippi Delta, Appalachia, and to a lesser extent the U.S. Mexican border are all home to clusters of high residuals. On the plus side, we don’t have any extreme outliers, so our model isn’t performing too badly! Produce a quick map to show this spatial distribution:</p>
<pre class="r"><code># Show R code to produce a basic map of residuals
# Show R code to produce choropleth map of PFHH variable

soco@data$res = resid(mod1)
# palette = colorRampPalette(c(&quot;white&quot;, &quot;red&quot;), space = &quot;rgb&quot;)
# spplot(soco, zcol=&quot;res&quot;, col.regions=palette(20), main=&quot;Global Moran&#39;s I&quot;, pretty=T, edge.col = &quot;transparent&quot;)

new_map = fortify(soco, region=&quot;FIPS&quot;)
class(new_map)  # Now we have a regular data.frame!</code></pre>
<pre><code>## [1] &quot;data.frame&quot;</code></pre>
<pre class="r"><code>new_map&lt;-merge(new_map,soco@data,by.x=&quot;id&quot;,by.y=&quot;FIPS&quot;) 
p = ggplot(data=new_map, aes(x=long, y=lat, group=group))
p + geom_polygon(aes(fill=res)) + coord_equal() + labs(x=&quot;Longitude&quot;, y=&quot;Latitude&quot;, title=&quot;Global Moran&#39;s I&quot;)  + scale_fill_gradient(&#39;PPOV&#39;, low = &quot;white&quot;, high = &quot;red&quot;, space = &quot;Lab&quot;, na.value = &quot;grey50&quot;, guide = &quot;colourbar&quot;)</code></pre>
<p><img src="ghandehari_lab8_files/figure-html/unnamed-chunk-21-1.png" title="" alt="" width="672" /></p>
<p>A good way to <em>quantitatively</em> show clustering in the above residuals might be to plot the local spatial autocorrelation statistics (Gi* or Moran’s Ii). For <strong>bonus</strong> points, compute the local spatial autocorrelation of the above residuals and map them:</p>
<pre class="r"><code># Bonus: compute local moran&#39;s Ii (or local getis and ord stat)
# Map results to get an idea of local clustering in residuals
lm1 = localmoran(resid(mod1),w_mat, zero.policy=TRUE)
soco@data$local_moran = (lm1[, &quot;Ii&quot;]) 

# palette = colorRampPalette(c(&quot;white&quot;, &quot;red&quot;), space = &quot;rgb&quot;)
# spplot(soco, zcol=&quot;local_moran&quot;, col.regions=palette(20), main=&quot;Local Moran&#39;s I&quot;, pretty=T, edge.col = &quot;transparent&quot;)

new_map = fortify(soco, region=&quot;FIPS&quot;)
class(new_map)  # Now we have a regular data.frame!</code></pre>
<pre><code>## [1] &quot;data.frame&quot;</code></pre>
<pre class="r"><code>new_map&lt;-merge(new_map,soco@data,by.x=&quot;id&quot;,by.y=&quot;FIPS&quot;) 
p = ggplot(data=new_map, aes(x=long, y=lat, group=group))
p + geom_polygon(aes(fill=local_moran)) + coord_equal() + labs(x=&quot;Longitude&quot;, y=&quot;Latitude&quot;, title=&quot;Local Moran&#39;s I&quot;)  + scale_fill_gradient(&#39;PPOV&#39;, low = &quot;white&quot;, high = &quot;red&quot;, space = &quot;Lab&quot;, na.value = &quot;grey50&quot;, guide = &quot;colourbar&quot;)</code></pre>
<p><img src="ghandehari_lab8_files/figure-html/unnamed-chunk-22-1.png" title="" alt="" width="672" /></p>
<pre class="r"><code>#####################

soco@data$local_moran_Z1 = (lm1[, &quot;Z.Ii&quot;]) # z-scores
# palette = colorRampPalette(c(&quot;white&quot;, &quot;red&quot;), space = &quot;rgb&quot;)
# spplot(soco, zcol=&quot;local_moran_Z1&quot;, col.regions=palette(20), main=&quot;Local Moran&#39;s I (z values)&quot;, pretty=T, edge.col = &quot;transparent&quot;)

new_map = fortify(soco, region=&quot;FIPS&quot;)
class(new_map)  # Now we have a regular data.frame!</code></pre>
<pre><code>## [1] &quot;data.frame&quot;</code></pre>
<pre class="r"><code>new_map&lt;-merge(new_map,soco@data,by.x=&quot;id&quot;,by.y=&quot;FIPS&quot;) 
p = ggplot(data=new_map, aes(x=long, y=lat, group=group))
p + geom_polygon(aes(fill=local_moran_Z1)) + coord_equal() + labs(x=&quot;Longitude&quot;, y=&quot;Latitude&quot;, title=&quot;Local Moran&#39;s I (z values)&quot;)  + scale_fill_gradient(&#39;PPOV&#39;, low = &quot;white&quot;, high = &quot;red&quot;, space = &quot;Lab&quot;, na.value = &quot;grey50&quot;, guide = &quot;colourbar&quot;)</code></pre>
<p><img src="ghandehari_lab8_files/figure-html/unnamed-chunk-22-2.png" title="" alt="" width="672" /></p>
<pre class="r"><code>#####################
lm4 = localG(resid(mod1),w_mat, zero.policy=TRUE)
soco@data$local_moran_Z1 = (as.numeric(lm4))
# palette = colorRampPalette(c(&quot;white&quot;, &quot;red&quot;), space = &quot;rgb&quot;)
# spplot(soco, zcol=&quot;local_moran_Z1&quot;, col.regions=palette(20), main=&quot;Local Getis-Ord G* (z values)&quot;, pretty=T, edge.col = &quot;transparent&quot;)

new_map = fortify(soco, region=&quot;FIPS&quot;)
class(new_map)  # Now we have a regular data.frame!</code></pre>
<pre><code>## [1] &quot;data.frame&quot;</code></pre>
<pre class="r"><code>new_map&lt;-merge(new_map,soco@data,by.x=&quot;id&quot;,by.y=&quot;FIPS&quot;) 
p = ggplot(data=new_map, aes(x=long, y=lat, group=group))
p + geom_polygon(aes(fill=local_moran_Z1)) + coord_equal() + labs(x=&quot;Longitude&quot;, y=&quot;Latitude&quot;, title=&quot;Local Moran&#39;s I (z values)&quot;)  + scale_fill_gradient(&#39;PPOV&#39;, low = &quot;white&quot;, high = &quot;red&quot;, space = &quot;Lab&quot;, na.value = &quot;grey50&quot;, guide = &quot;colourbar&quot;)</code></pre>
<p><img src="ghandehari_lab8_files/figure-html/unnamed-chunk-22-3.png" title="" alt="" width="672" /></p>
</div>
</div>
</div>
<div id="part-ii-spatial-regression" class="section level1">
<h1>Part II: Spatial Regression</h1>
<p>In this segment we will run spatial lag and spatial error models and compare the results we will also work at interpreting the models.</p>
<p>We begin, as before with specifying our regression model design: <code>SQRTPPOV ~ PHSP + PFHH + PUNEM + PEXTR + P65UP + METRO + PHSPLUS</code>. We’ll use the same weights matrix as before (<code>w_mat</code>), but this time we’ll fit spatial lag and error models:</p>
<pre class="r"><code># Show R code to fit a spatial lag (lag_mod) and a spatial error (err_mod) model

lag_mod = lagsarlm (SQRTPPOV  ~ PHSP + PFHH + PUNEM + PEXTR + P65UP + METRO + PHSPLUS, data = soco@data, w_mat)
summary(lag_mod)</code></pre>
<pre><code>## 
## Call:lagsarlm(formula = SQRTPPOV ~ PHSP + PFHH + PUNEM + PEXTR + P65UP + 
##     METRO + PHSPLUS, data = soco@data, listw = w_mat)
## 
## Residuals:
##        Min         1Q     Median         3Q        Max 
## -0.2399004 -0.0243576  0.0011752  0.0237824  0.1596573 
## 
## Type: lag 
## Coefficients: (asymptotic standard errors) 
##               Estimate Std. Error  z value  Pr(&gt;|z|)
## (Intercept)  0.1851461  0.0111780  16.5634 &lt; 2.2e-16
## PHSP         0.0641592  0.0089201   7.1926 6.355e-13
## PFHH         0.4596390  0.0203098  22.6314 &lt; 2.2e-16
## PUNEM        1.0615394  0.0598353  17.7410 &lt; 2.2e-16
## PEXTR        0.2397735  0.0236232  10.1499 &lt; 2.2e-16
## P65UP        0.2189071  0.0320331   6.8338 8.271e-12
## METRO       -0.0075423  0.0028620  -2.6353  0.008406
## PHSPLUS     -0.2427931  0.0125381 -19.3645 &lt; 2.2e-16
## 
## Rho: 0.334, LR test value: 266.95, p-value: &lt; 2.22e-16
## Asymptotic standard error: 0.020031
##     z-value: 16.674, p-value: &lt; 2.22e-16
## Wald statistic: 278.04, p-value: &lt; 2.22e-16
## 
## Log likelihood: 2457.162 for lag model
## ML residual variance (sigma squared): 0.0016571, (sigma: 0.040708)
## Number of observations: 1387 
## Number of parameters estimated: 10 
## AIC: -4894.3, (AIC for lm: -4629.4)
## LM test for residual autocorrelation
## test value: 48.739, p-value: 2.9233e-12</code></pre>
<pre class="r"><code>err_mod = errorsarlm (SQRTPPOV  ~ PHSP + PFHH + PUNEM + PEXTR + P65UP + METRO + PHSPLUS, data = soco@data, w_mat)
summary(err_mod)</code></pre>
<pre><code>## 
## Call:errorsarlm(formula = SQRTPPOV ~ PHSP + PFHH + PUNEM + PEXTR + 
##     P65UP + METRO + PHSPLUS, data = soco@data, listw = w_mat)
## 
## Residuals:
##        Min         1Q     Median         3Q        Max 
## -0.2197259 -0.0219593  0.0009531  0.0225722  0.1641473 
## 
## Type: error 
## Coefficients: (asymptotic standard errors) 
##               Estimate Std. Error  z value  Pr(&gt;|z|)
## (Intercept)  0.3004974  0.0107069  28.0658 &lt; 2.2e-16
## PHSP         0.0990182  0.0162385   6.0977 1.076e-09
## PFHH         0.6594265  0.0229819  28.6933 &lt; 2.2e-16
## PUNEM        0.8934878  0.0623241  14.3361 &lt; 2.2e-16
## PEXTR        0.3093215  0.0275166  11.2413 &lt; 2.2e-16
## P65UP        0.2128108  0.0387013   5.4988 3.824e-08
## METRO       -0.0045409  0.0028383  -1.5999    0.1096
## PHSPLUS     -0.2479096  0.0129779 -19.1025 &lt; 2.2e-16
## 
## Lambda: 0.6607, LR test value: 362.28, p-value: &lt; 2.22e-16
## Asymptotic standard error: 0.025914
##     z-value: 25.496, p-value: &lt; 2.22e-16
## Wald statistic: 650.07, p-value: &lt; 2.22e-16
## 
## Log likelihood: 2504.829 for error model
## ML residual variance (sigma squared): 0.0014297, (sigma: 0.037811)
## Number of observations: 1387 
## Number of parameters estimated: 10 
## AIC: -4989.7, (AIC for lm: -4629.4)</code></pre>
<p>Now comes the fun part, interpreting our results. We can print the <code>summary()</code> results of all three model runs and explore the output. We have already started with the first model (OLS) and explored some spatial dependence diagnostics. Indeed, we have already seen that the LM’s and robust LM’s indicate we should prefer a spatial <em>lag</em> model over the spatial <em>error</em> model.</p>
<p>Now let’s compare the summary model diagnostics. The <span class="math">\(R^2\)</span> value is a bit ‘iffy’ with spatial models, so the log-likelihood and AIC are preferred. Which model appears to perform best in terms of model fit?</p>
<pre class="r"><code># Show some R code to highlight which model fits best in terms of R^2, AIC, and log-likelihood
lag_mod$AIC_lm.model</code></pre>
<pre><code>## [1] -4629.376</code></pre>
<pre class="r"><code>lag_mod$logLik_lm.model</code></pre>
<pre><code>## &#39;log Lik.&#39; 2323.688 (df=9)</code></pre>
<pre class="r"><code># for lag model
#Log likelihood: 2457.162
#AIC: -4894.3

# for error model
#Log likelihood: 2504.829
#AIC: -4989.7

#AIC of spatial models is lower than the linear model, means a better model fit.

#test value: 48.739, p-value: 2.9233e-12
#LM test indicate there is a significant spatial autocorrelation in the residuals

#So AIC and log-likelihood show that spatial models is better than a non-spatial linera model. Also it seems that error molde works better than log model due to the higher value of Log likelihood and lower value of AIC.</code></pre>
<p>Turn next to the spatial autoregressive coefficients (<span class="math">\(\rho\)</span>, spatial lag, or <span class="math">\(\lambda\)</span>, spatial error). What is the value of Rho? Is it significant? What is its sign (positive or negative spatial autocorrelation)?</p>
<pre class="r"><code># Show R code to get Rho (or just refer to earlier printout and state value)
# Is it significant? What is its sign? What does this mean?
lag_mod$rho</code></pre>
<pre><code>##      rho 
## 0.334002</code></pre>
<pre class="r"><code>#Rho: 0.334, LR test value: 266.95, p-value: &lt; 2.22e-16
#Rho is positive and highly significant; It means that there is spatial dependence in our data. By the way this value indicates a strong influence on observations by their neighboring observations.</code></pre>
<p>Show the same for the error model. Which one seems to indicate stronger spatial autocorrelation? Is there a difference in the associated standar errors for these coefficients?</p>
<pre class="r"><code># Show R code to get Lambda (or just refer to earlier printout and state value)
# Is it significant? How does it compare to above value for Rho?
err_mod$lambda</code></pre>
<pre><code>##   lambda 
## 0.660705</code></pre>
<pre class="r"><code>#Lambda: 0.6607, LR test value: 362.28, p-value: &lt; 2.22e-16
#Lambda is positive and significant; It means there is spatial autocorrelation leads to residual dependence
# Lambda is hgier than rho. We cannot compare this two parameters directly because Pho is a substantive parameter, but Lambda is a nuisance parameter</code></pre>
<p>What about the LR and Wald tests? What do these suggest? How do they compare to ANOVA between the spatial regression models and the OLS model?</p>
<pre class="r"><code># Show R code to compare models&#39; LR and Wald tests (or refer to earlier printout)
# How is this similar/dis-misimilar to an ANOVA test?
# lag_mod
# LR test value: 266.95, p-value: &lt; 2.22e-16
# Wald statistic: 278.04, p-value: &lt; 2.22e-16
#These are tests for the significance of the spatial term. LR test check to see if a spatial model is better than a non- spatial model. Here this test is significant and so a spatial model is better than a non- spatial model.

# 
# err_mod
# LR test value: 362.28, p-value: &lt; 2.22e-16
# Wald statistic: 650.07, p-value: &lt; 2.22e-
#These are tests for the significance of the spatial error term. LR test is a test of the sinificance of the spatial term. As it is significant here, the spatial term improve the model. in this respect it is a equivalent to anova. Wald statistic is like testing a full and reduced model. It tests a model with and without the error term. Here it&#39;s significant; so spatial terms improve the model.</code></pre>
<p>Look at other explanatory variables (signs and magnitudes). It looks like the <code>METRO</code> variable lost significance in the error model. What might this suggest? To get a better idea of the <em>impacts</em> that these models are capturing, take at look at the direct, indirect, and total impacts of the lag model:</p>
<pre class="r"><code># R code to extract &#39;impacts&#39; of the lag model (caution, very slow!)
#Impacts of Lag Model
#METRO has a small direct nad indirect impact in comparison to the other variables; So We may be able to eliminate this parameter from our model
impacts(lag_mod, listw=w_mat)</code></pre>
<pre><code>## Impact measures (lag, exact):
##               Direct     Indirect       Total
## PHSP     0.065638554  0.030696942  0.09633550
## PFHH     0.470236814  0.219913927  0.69015074
## PUNEM    1.086015159  0.507892728  1.59390789
## PEXTR    0.245301895  0.114719438  0.36002133
## P65UP    0.223954420  0.104735943  0.32869036
## METRO   -0.007716214 -0.003608614 -0.01132483
## PHSPLUS -0.248391126 -0.116164167 -0.36455529</code></pre>
<p>How do the above impacts relate to the values from the OLS and error model?</p>
<pre class="r"><code># Describe the key differences between the three models in
# terms of coefficients
# Reading coefficients in the spatial models is not straight forward. In OLS each coefficient can be diretly interpreted, but in spatial models it&#39;s more complicated. In the lag model one unit chage in one variable would influence the whole system. So, for interpreting these coefficient sth called spillover effects is used. Here one unit change can have some direct impacts in one location and some icdirect ompacts on the rest of the map. </code></pre>
<p>Our model <em>still</em> has major problems with heteroskedasticity. We would have to deal with this going forward. Consider a scatterplot that compares predicted values against residuals:</p>
<pre class="r"><code># Show R code to produce a scatterplot to help us
# compare predicted with residuals values from our
# chosen model
plot(resid(err_mod) ~ predict(err_mod))</code></pre>
<p><img src="ghandehari_lab8_files/figure-html/unnamed-chunk-30-1.png" title="" alt="" width="672" /></p>
<p>What about a Breusch-Pagan test to look at heteroscedasticity? In addition to the measure we used earlier, we can use the <code>bptest.sarlm</code> function from the <code>spdep</code> package to look at heteroscedasticity in spatial models <em>specifically</em> (perform for both lag and error model and discuss significance):</p>
<pre class="r"><code># R code to perform BP test on spatial regression models.
# Are they significant?
# p-value is quite small and BP test on both lag model and error model is significant. So we have heteroscedasticity in our spatial models.
bptest.sarlm(lag_mod)</code></pre>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  
## BP = 90.192, df = 7, p-value &lt; 2.2e-16</code></pre>
<pre class="r"><code>bptest.sarlm(err_mod)</code></pre>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  
## BP = 143.35, df = 7, p-value &lt; 2.2e-16</code></pre>
<p>For bonus points, take a look at the remaining residual spatial autocorrelation. Is it significant?</p>
<pre class="r"><code># Bonus: Compute global and local spatial autocorrelation in
# the error model residuals.
# Is the SA still significant?
# Consider plotting the local spatial autocorrelation also...

plot(err_mod$resid)</code></pre>
<p><img src="ghandehari_lab8_files/figure-html/unnamed-chunk-32-1.png" title="" alt="" width="672" /></p>
<pre class="r"><code>hist(resid(err_mod))</code></pre>
<p><img src="ghandehari_lab8_files/figure-html/unnamed-chunk-32-2.png" title="" alt="" width="672" /></p>
<pre class="r"><code>#map residuals
soco@data$res = resid(err_mod)
new_map = fortify(soco, region=&quot;FIPS&quot;)
class(new_map)  # Now we have a regular data.frame!</code></pre>
<pre><code>## [1] &quot;data.frame&quot;</code></pre>
<pre class="r"><code>new_map&lt;-merge(new_map,soco@data,by.x=&quot;id&quot;,by.y=&quot;FIPS&quot;) 
p = ggplot(data=new_map, aes(x=long, y=lat, group=group))
p + geom_polygon(aes(fill=res)) + coord_equal() + labs(x=&quot;Longitude&quot;, y=&quot;Latitude&quot;, title=&quot;residuals&quot;)  + scale_fill_gradient(&#39;PPOV&#39;, low = &quot;white&quot;, high = &quot;red&quot;, space = &quot;Lab&quot;, na.value = &quot;grey50&quot;, guide = &quot;colourbar&quot;)</code></pre>
<p><img src="ghandehari_lab8_files/figure-html/unnamed-chunk-32-3.png" title="" alt="" width="672" /></p>
<pre class="r"><code>#####################
moran.test(resid(err_mod),w_mat, zero.policy=T) #Moran&#39;s I</code></pre>
<pre><code>## 
##  Moran I test under randomisation
## 
## data:  resid(err_mod)  
## weights: w_mat  
## 
## Moran I statistic standard deviate = -2.888, p-value = 0.9981
## alternative hypothesis: greater
## sample estimates:
## Moran I statistic       Expectation          Variance 
##     -0.0473250517     -0.0007215007      0.0002604038</code></pre>
<pre class="r"><code>#So the resildiuals are not autocorrelated
#####################
lm4 = localG(resid(err_mod),w_mat, zero.policy=TRUE)
soco@data$local_moran_Z1 = (as.numeric(lm4))
# palette = colorRampPalette(c(&quot;white&quot;, &quot;red&quot;), space = &quot;rgb&quot;)
# spplot(soco, zcol=&quot;local_moran_Z1&quot;, col.regions=palette(20), main=&quot;Local Getis-Ord G* (z values)&quot;, pretty=T, edge.col = &quot;transparent&quot;)

new_map = fortify(soco, region=&quot;FIPS&quot;)
class(new_map)  # Now we have a regular data.frame!</code></pre>
<pre><code>## [1] &quot;data.frame&quot;</code></pre>
<pre class="r"><code>new_map&lt;-merge(new_map,soco@data,by.x=&quot;id&quot;,by.y=&quot;FIPS&quot;) 
p = ggplot(data=new_map, aes(x=long, y=lat, group=group))
p + geom_polygon(aes(fill=local_moran_Z1)) + coord_equal() + labs(x=&quot;Longitude&quot;, y=&quot;Latitude&quot;, title=&quot;Local Moran&#39;s I (z values)&quot;)  + scale_fill_gradient(&#39;PPOV&#39;, low = &quot;white&quot;, high = &quot;red&quot;, space = &quot;Lab&quot;, na.value = &quot;grey50&quot;, guide = &quot;colourbar&quot;)</code></pre>
<p><img src="ghandehari_lab8_files/figure-html/unnamed-chunk-32-4.png" title="" alt="" width="672" /></p>
<pre class="r"><code>########
lm1 = localmoran(resid(err_mod),w_mat, zero.policy=TRUE)
# Create a LISA Cluster Map
# reference: http://isites.harvard.edu/fs/docs/icb.topic923307.files/R%20code%20for%20Lab%20Ex%206.txt

quadrant &lt;- vector(mode=&quot;numeric&quot;,length=1386)

# centers the variable of interest around its mean
cCMEDV = (lag.listw(w_mat,resid(err_mod))) - mean(resid(err_mod))
#cCMEDV &lt;- data.election$Bush_pct - mean(data.election$Bush_pct)  

# centers the Z value local Moran&#39;s around the mean
C_mI &lt;- lm1[,4]  

signif &lt;- 0.1       
# set a statistical significance level for the local Moran&#39;s

quadrant[cCMEDV &gt;0 &amp; C_mI&gt;0] &lt;- 4      
# these four command lines define the high-high, low-low
# low-high and high-low categories

quadrant[cCMEDV &lt;0 &amp; C_mI&gt;0] &lt;- 1      
quadrant[cCMEDV &lt;0 &amp; C_mI&lt;0] &lt;- 2
quadrant[cCMEDV &gt;0 &amp; C_mI&lt;0] &lt;- 3
quadrant[lm1[,5]&gt;signif] &lt;- 0     
# places non-significant Moran&#39;s in the category &quot;0&quot;

brks &lt;- c(0,1,2,3,4)
colors &lt;- c(&quot;white&quot;,&quot;blue&quot;,rgb(0,0,1,alpha=0.4),rgb(1,0,0,alpha=0.4),&quot;red&quot;)
plot(soco,border=&quot;lightgray&quot;,col=colors[findInterval(quadrant,brks,all.inside=FALSE)])
box()
legend(&quot;bottomright&quot;,legend=c(&quot;insignificant&quot;,&quot;low-low&quot;,&quot;low-high&quot;,&quot;high-low&quot;,&quot;high-high&quot;),
       fill=colors,bty=&quot;n&quot;,cex=0.7,y.intersp=1,x.intersp=1)
title(&quot;LISA Cluster Map&quot;)</code></pre>
<p><img src="ghandehari_lab8_files/figure-html/unnamed-chunk-32-5.png" title="" alt="" width="672" /></p>
<p>Stepping back a little, let’s try and understand what these models have told us:</p>
<ul>
<li>First, there are spatial processes at play in our data that we need to be thinking about—ignoring spatial autocorrelation of over 0.3 is not good practice!</li>
<li>Second, our diagnostics and output consistently favor a Spatial <em>Error</em> form as a means of capturing this spatial autocorrelation.</li>
</ul>
<p>This suggests that our processes are varying consistently across small areas, but that we are not likely seeing an active process of counties interacting with one another—so we don’t need to talk about the movement of individuals across county lines as the source of this relationship—but we <em>are</em> more likely to have some combination of large scale regional processes and regionally varying missing variables.</p>
<div id="reference" class="section level3">
<h3>Reference</h3>
<p>This lab has been adapted from a lab developed by <a href="mailto:csfowler@uw.edu">Chris Fowler</a> from the University of Washington.</p>
<hr />
<p>library(knitr) rmarkdown::render(‘ghandehari_lab8.Rmd’)</p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
